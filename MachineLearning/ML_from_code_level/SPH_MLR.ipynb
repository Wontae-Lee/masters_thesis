{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d7598ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T03:11:24.279475083Z",
     "start_time": "2023-10-29T03:11:23.473170340Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import tensorflow as tf\n",
    "# import os\n",
    "# import matplotlib.pyplot as plt\n",
    "# from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6c0792b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T03:11:24.423599847Z",
     "start_time": "2023-10-29T03:11:24.282120097Z"
    }
   },
   "outputs": [],
   "source": [
    "npInputDataset = np.load(f\"./../../Solver/DATA/npInputDataset.npy\")\n",
    "npXTargetDataset= np.load(f\"./../../Solver/DATA/npXTargetDataset.npy\")\n",
    "npYTargetDataset= np.load(f\"./../../Solver/DATA/npYTargetDataset.npy\")\n",
    "npUTargetDataset= np.load(f\"./../../Solver/DATA/npUTargetDataset.npy\")\n",
    "npVTargetDataset= np.load(f\"./../../Solver/DATA/npVTargetDataset.npy\")\n",
    "npAccXTargetDataset = np.load(f\"./../../Solver/DATA/npXTargetDataset.npy\")\n",
    "npAccYTargetDataset= np.load(f\"./../../Solver/DATA/npAccYTargetDataset.npy\")\n",
    "npRhoTargetDataset = np.load(f\"./../../Solver/DATA/npRhoTargetDataset.npy\")\n",
    "npPTargetDataset = np.load(f\"./../../Solver/DATA/npPTargetDataset.npy\")\n",
    "npCollidingTotal = np.load(f\"./../../Solver/DATA/npCollidingTotal.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecbd162c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T03:11:24.436128099Z",
     "start_time": "2023-10-29T03:11:24.426465984Z"
    }
   },
   "outputs": [],
   "source": [
    "npXTargetDataset = np.expand_dims(npXTargetDataset, axis=-1)\n",
    "npYTargetDataset= np.expand_dims(npYTargetDataset, axis=-1)\n",
    "npUTargetDataset= np.expand_dims(npUTargetDataset, axis=-1)\n",
    "npVTargetDataset= np.expand_dims(npVTargetDataset, axis=-1)\n",
    "npAccXTargetDataset = np.expand_dims(npAccXTargetDataset, axis=-1)\n",
    "npAccYTargetDataset= np.expand_dims(npAccYTargetDataset, axis=-1)\n",
    "npRhoTargetDataset =np.expand_dims(npRhoTargetDataset, axis=-1)\n",
    "npPTargetDataset = np.expand_dims(npPTargetDataset, axis=-1)\n",
    "npCollidingTotal = np.expand_dims(npCollidingTotal,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7acf3479",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T03:11:24.486342658Z",
     "start_time": "2023-10-29T03:11:24.431112202Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "RandomState(MT19937) at 0x7F42C48EAD40"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.RandomState(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a395403",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T03:11:24.486700253Z",
     "start_time": "2023-10-29T03:11:24.477866263Z"
    }
   },
   "outputs": [],
   "source": [
    "# The number of data\n",
    "NData = 100\n",
    "\n",
    "# The number of particles\n",
    "NParticles = 100\n",
    "\n",
    "# The number of TimeSteps\n",
    "NTimeSteps = 200\n",
    "\n",
    "# The number of parameters\n",
    "NParameters = 4\n",
    "\n",
    "# The number of Targets\n",
    "NTargets = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235ae1b6",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1046968e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T03:11:24.486852440Z",
     "start_time": "2023-10-29T03:11:24.478155551Z"
    }
   },
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, targetTitle):\n",
    "        \n",
    "        self.targetTitle = targetTitle\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.losses = np.array([])\n",
    "        self.val_losses = np.array([])\n",
    "        self.predict_losses = np.array([])\n",
    "        self.w_history = None\n",
    "        self.learningRate = None\n",
    "        self.timeStep = None\n",
    "        self.NDatas = None\n",
    "        self.NParticles = None\n",
    "        self.NParameters = None\n",
    "        self.NTargets = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.X_Val = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.y_val = None\n",
    "\n",
    "    def setup(self, inputDataset, targetDataset, randW, randb,test_size=0.25, random_state=42, learningRate=0.001):\n",
    "        \n",
    "        # Variables\n",
    "        self.timeStep = inputDataset.shape[1]\n",
    "        self.NDatas = inputDataset.shape[0] \n",
    "        self.NParticles = inputDataset.shape[2]\n",
    "        self.NParameters = inputDataset.shape[3]\n",
    "        self.NTargets = targetDataset.shape[0]\n",
    "        self.learningRate = learningRate\n",
    "        \n",
    "        self.w = randW\n",
    "        self.b = randb\n",
    "        \n",
    "               \n",
    "        # Dataset splitting\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(inputDataset, targetDataset,\n",
    "                                                                                test_size=test_size,\n",
    "                                                                                random_state=random_state)\n",
    "\n",
    "        \n",
    "        \n",
    "        self.X_train, self.X_Val, self.y_train, self.y_val = train_test_split(self.X_train, self.y_train,\n",
    "                                                                              test_size=test_size,\n",
    "                                                                              random_state=random_state)\n",
    "        \n",
    "        # Sklearn Input Dataset preprocessing\n",
    "        # Triainset\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        scaler =MinMaxScaler()\n",
    "        NTrainData = self.X_train.shape[0]\n",
    "        self.X_train = self.X_train.reshape(-1,self.NParameters)\n",
    "        self.X_train = scaler.fit_transform(self.X_train)\n",
    "        self.X_train = self.X_train.reshape(NTrainData,self.timeStep,self.NParticles,self.NParameters)\n",
    "        # Testset\n",
    "        NTestData = self.X_test.shape[0]\n",
    "        self.X_test = self.X_test.reshape(-1, self.NParameters)\n",
    "        self.X_test = scaler.transform(self.X_test)\n",
    "        self.X_test = self.X_test.reshape(NTestData,self.timeStep,self.NParticles,self.NParameters)\n",
    "        \n",
    "\n",
    "    def forpass(self, x):\n",
    "        y_hat = x * self.w + self.b\n",
    "        return y_hat\n",
    "\n",
    "    def backprop(self, x, err):\n",
    "        w_grad = x * err\n",
    "        b_grad = 1 * err\n",
    "        return w_grad, b_grad\n",
    "\n",
    "    def update_val_loss(self):\n",
    "\n",
    "\n",
    "        y_hat = self.forpass(self.X_Val)\n",
    "        y_hat = np.sum(y_hat, axis=-1)\n",
    "        y_hat = np.expand_dims(y_hat, axis=-1)\n",
    "        err = -(self.y_val - y_hat)\n",
    "        self.val_losses = np.append(self.val_losses, np.sum(err))\n",
    "\n",
    "        \n",
    "    def prediction(self):\n",
    "        \n",
    "        NTest = self.X_test.shape[0]\n",
    "        y_hat = self.forpass(self.X_test)\n",
    "        y_hat = np.sum(y_hat, axis=-1)\n",
    "        y_hat = np.expand_dims(y_hat, axis=-1)\n",
    "        \n",
    "        self.predict_losses = -(self.y_test - y_hat)\n",
    "    \n",
    "    def saveResults(self,address,modelName):\n",
    "        \n",
    "        np.save(f\"{address}/{modelName}weights.npy\",self.w)\n",
    "        np.save(f\"{address}/{modelName}bias.npy\",self.b)\n",
    "        np.save(f\"{address}/{modelName}losses.npy\",self.losses)\n",
    "        np.save(f\"{address}/{modelName}val_losses.npy\",self.val_losses)\n",
    "        np.save(f\"{address}/{modelName}predict_losses.npy\",self.predict_losses)\n",
    "        np.save(f\"{address}/{modelName}X_train.npy\",self.X_train)\n",
    "        np.save(f\"{address}/{modelName}X_test.npy\",self.X_test)\n",
    "        np.save(f\"{address}/{modelName}X_Val.npy\",self.X_Val)\n",
    "        np.save(f\"{address}/{modelName}y_train.npy\",self.y_train)\n",
    "        np.save(f\"{address}/{modelName}y_test.npy\",self.y_test)\n",
    "        np.save(f\"{address}/{modelName}y_val.npy\",self.y_val)\n",
    "\n",
    "    def fit(self, epochs=10):\n",
    "        \n",
    "        self.w_history = self.w\n",
    "        np.random.seed(42)\n",
    "        indexes = np.random.permutation(np.arange(self.X_train.shape[0]))\n",
    "        for _ in range(epochs):\n",
    "            loss = 0\n",
    "            for i in indexes:                \n",
    "                y_hat = self.forpass(self.X_train[i])\n",
    "                y_hat = np.sum(y_hat, axis=-1)\n",
    "                y_hat = np.expand_dims(y_hat, axis=-1)\n",
    "                err = -(self.y_train[i] - y_hat)\n",
    "                w_grad, b_grad = self.backprop(self.X_train[i], err)\n",
    "                self.w -= w_grad * self.learningRate\n",
    "                self.b -= b_grad * self.learningRate\n",
    "                loss += np.sum(err)\n",
    "            self.losses = np.append(self.losses, loss)\n",
    "            self.update_val_loss()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2766ce38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T03:11:24.486993592Z",
     "start_time": "2023-10-29T03:11:24.478430522Z"
    }
   },
   "outputs": [],
   "source": [
    "# Epochs\n",
    "Epochs = 3000\n",
    "\n",
    "# stadard deviation of random initiai weights\n",
    "stdScale = 0.01\n",
    "\n",
    "\n",
    "# Model 1\n",
    "Model1Epochs = 3000\n",
    "Model1LearningRate = 0.001\n",
    "\n",
    "# Model 2\n",
    "Model2Epochs = 3000\n",
    "Model2LearningRate = 0.001\n",
    "\n",
    "# Model 3\n",
    "Model3Epochs = 3000\n",
    "Model3LearningRate = 0.001\n",
    "\n",
    "# Model 4\n",
    "Model4Epochs = 3000\n",
    "Model4LearningRate = 0.001\n",
    "\n",
    "# Model 5\n",
    "Model5Epochs = 3000\n",
    "Model5LearningRate = 0.001\n",
    "\n",
    "# Model 6\n",
    "Model6Epochs = 3000\n",
    "Model6LearningRate = 0.001\n",
    "\n",
    "# Model 7\n",
    "Model7Epochs = 3000\n",
    "Model7LearningRate = 0.001\n",
    "\n",
    "# Model 8\n",
    "Model8Epochs = 3000\n",
    "Model8LearningRate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f088affe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T03:14:26.415534655Z",
     "start_time": "2023-10-29T03:11:24.478620844Z"
    }
   },
   "outputs": [],
   "source": [
    "Model1 = Neuron(targetTitle=\"The x-coordinates of the particles[m]\")\n",
    "Model1.setup(npInputDataset,npXTargetDataset,\n",
    "             randW = np.random.normal(scale= stdScale ,size=(NTimeSteps,NParticles,NParameters)),\n",
    "             randb = np.zeros(shape=(NTimeSteps,NParticles,NParameters)),\n",
    "             learningRate=Model1LearningRate)\n",
    "\n",
    "Model1.fit(epochs=Model1Epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e768ac7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T03:17:23.713819173Z",
     "start_time": "2023-10-29T03:14:26.415914110Z"
    }
   },
   "outputs": [],
   "source": [
    "Model2 = Neuron(targetTitle=\"The y-coordinates of the particles[m]\")\n",
    "Model2.setup(npInputDataset,npYTargetDataset,\n",
    "             randW = np.random.normal(scale=  stdScale ,size=(NTimeSteps,NParticles,NParameters)),\n",
    "             randb = np.zeros(shape=(NTimeSteps,NParticles,NParameters)),\n",
    "             learningRate=Model2LearningRate)\n",
    "\n",
    "Model2.fit(epochs=Model2Epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08e426c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T03:20:15.655995269Z",
     "start_time": "2023-10-29T03:17:23.717161558Z"
    }
   },
   "outputs": [],
   "source": [
    "Model3 = Neuron(targetTitle=\"The x-direction velocity of the particles[m/s]\")\n",
    "Model3.setup(npInputDataset,npUTargetDataset,\n",
    "            randW = np.random.normal(scale=  stdScale ,size=(NTimeSteps,NParticles,NParameters)),\n",
    "             randb = np.zeros(shape=(NTimeSteps,NParticles,NParameters)),\n",
    "             learningRate=Model3LearningRate)\n",
    "Model3.fit(epochs=Model3Epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55b552cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T03:23:03.360783072Z",
     "start_time": "2023-10-29T03:20:15.701532837Z"
    }
   },
   "outputs": [],
   "source": [
    "Model4 = Neuron(targetTitle=\"The y-direction velocity of the particles[m/s]\")\n",
    "Model4.setup(npInputDataset,npVTargetDataset,\n",
    "             randW = np.random.normal(scale= stdScale ,size=(NTimeSteps,NParticles,NParameters)),\n",
    "             randb = np.zeros(shape=(NTimeSteps,NParticles,NParameters)),\n",
    "             learningRate=Model4LearningRate)\n",
    "Model4.fit(epochs=Model4Epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8334015",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T03:25:51.978540658Z",
     "start_time": "2023-10-29T03:23:03.401692505Z"
    }
   },
   "outputs": [],
   "source": [
    "Model5 = Neuron(targetTitle=\"The x-direction accelerlation of the particles[m/s^2]\")\n",
    "Model5.setup(npInputDataset,npAccXTargetDataset,\n",
    "             randW = np.random.normal(scale= stdScale ,size=(NTimeSteps,NParticles,NParameters)),\n",
    "             randb = np.zeros(shape=(NTimeSteps,NParticles,NParameters)),\n",
    "             learningRate=Model5LearningRate)\n",
    "\n",
    "Model5.fit(epochs=Model5Epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f99940c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T03:28:38.964257798Z",
     "start_time": "2023-10-29T03:25:51.980873044Z"
    }
   },
   "outputs": [],
   "source": [
    "Model6 = Neuron(targetTitle=\"The y-direction accelerlation of the particles[m/s^2]\")\n",
    "Model6.setup(npInputDataset,npAccYTargetDataset,\n",
    "              randW = np.random.normal(scale= stdScale ,size=(NTimeSteps,NParticles,NParameters)),\n",
    "             randb = np.zeros(shape=(NTimeSteps,NParticles,NParameters)),\n",
    "             learningRate=Model6LearningRate)\n",
    "Model6.fit(epochs=Model6Epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cf698ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T03:31:44.221820994Z",
     "start_time": "2023-10-29T03:28:38.966804391Z"
    }
   },
   "outputs": [],
   "source": [
    "Model7 = Neuron(targetTitle=\"The density of particles[kg/m^3]\")\n",
    "Model7.setup(npInputDataset,npRhoTargetDataset,\n",
    "              randW = np.random.normal(scale= stdScale ,size=(NTimeSteps,NParticles,NParameters)),\n",
    "             randb = np.zeros(shape=(NTimeSteps,NParticles,NParameters)),\n",
    "             learningRate=Model7LearningRate)\n",
    "\n",
    "Model7.fit(epochs=Model7Epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36d6fe65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T03:34:31.273848268Z",
     "start_time": "2023-10-29T03:31:44.224501265Z"
    }
   },
   "outputs": [],
   "source": [
    "Model8 = Neuron(targetTitle=\"The pressure of particles[Pa]\")\n",
    "Model8.setup(npInputDataset,npPTargetDataset,\n",
    "               randW = np.random.normal(scale= stdScale ,size=(NTimeSteps,NParticles,NParameters)),\n",
    "             randb = np.zeros(shape=(NTimeSteps,NParticles,NParameters)),\n",
    "             learningRate=Model8LearningRate)\n",
    "Model8.fit(epochs=Model8Epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c141f670",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf84b16d",
   "metadata": {},
   "source": [
    "#### Epochs: 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8a009024",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T03:47:02.494941281Z",
     "start_time": "2023-10-29T03:47:02.450791470Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "SaveAddress = \"./Results\"\n",
    "saveAddressValLosses = \"Results_3000epochs_std_set_0.01_valLosses.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5a541916",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T03:47:02.957294392Z",
     "start_time": "2023-10-29T03:47:02.764620210Z"
    }
   },
   "outputs": [],
   "source": [
    "Model1.prediction()\n",
    "Model2.prediction()\n",
    "Model3.prediction()\n",
    "Model4.prediction()\n",
    "Model5.prediction()\n",
    "Model6.prediction()\n",
    "Model7.prediction()\n",
    "Model8.prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c1331e5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T03:47:03.863486368Z",
     "start_time": "2023-10-29T03:47:03.128227689Z"
    }
   },
   "outputs": [],
   "source": [
    "Model1.saveResults(SaveAddress ,\"Model1\")\n",
    "Model2.saveResults(SaveAddress ,\"Model2\")\n",
    "Model3.saveResults(SaveAddress ,\"Model3\")\n",
    "Model4.saveResults(SaveAddress ,\"Model4\")\n",
    "Model5.saveResults(SaveAddress ,\"Model5\")\n",
    "Model6.saveResults(SaveAddress ,\"Model6\")\n",
    "Model7.saveResults(SaveAddress ,\"Model7\")\n",
    "Model8.saveResults(SaveAddress ,\"Model8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "069a097f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T03:47:05.055534817Z",
     "start_time": "2023-10-29T03:47:04.406814058Z"
    }
   },
   "outputs": [],
   "source": [
    "dfValLosses = np.abs(np.hstack([Model1.val_losses.reshape(Epochs,-1),Model2.val_losses.reshape(Epochs,-1),\n",
    "                         Model3.val_losses.reshape(Epochs,-1),Model4.val_losses.reshape(Epochs,-1),\n",
    "                         Model5.val_losses.reshape(Epochs,-1),Model6.val_losses.reshape(Epochs,-1),\n",
    "                         Model7.val_losses.reshape(Epochs,-1),Model8.val_losses.reshape(Epochs,-1)]))\n",
    "dfValLosses = pd.DataFrame(dfValLosses)\n",
    "dfValLosses.to_excel(f\"{SaveAddress}/{saveAddressValLosses}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c9408e7d0fde28fe"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
